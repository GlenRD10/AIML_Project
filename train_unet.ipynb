{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_unet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeeJLTJo7-QC",
        "outputId": "da2adae3-1edf-4026-b4f5-f24ab0eb75ba"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFXSCEsVNsR5"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pathlib\n",
        "import logging\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "import h5py\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnx4on7h3UMD"
      },
      "source": [
        "args = {\n",
        "    'seed': 42,\n",
        "    'resolution': 320,\n",
        "    'challenge': 'singlecoil',\n",
        "    'data_path': pathlib.Path('/content/drive/MyDrive/Dataset'),\n",
        "    'sample_rate': 1.,\n",
        "    'accelerations': [4, 8],\n",
        "    'center_fractions': [0.08, 0.04],\n",
        "\n",
        "    'num_pools': 4,\n",
        "    'drop_prob': 0.0,\n",
        "    'num_chans': 32,\n",
        "    'batch_size': 16,\n",
        "    'num_epochs': 2,\n",
        "    'lr': 0.001,\n",
        "    'lr_step_size': 40,\n",
        "    'lr_gamma': 0.1,\n",
        "    'weight_decay': 0.,\n",
        "    'report_interval': 100,\n",
        "    'data_parallel': False,\n",
        "    'device': 'cuda',\n",
        "    'exp_dir': pathlib.Path('/content/drive/MyDrive/checkpoints'),\n",
        "    'resume': True,\n",
        "    'checkpoint': pathlib.Path('/content/drive/MyDrive/checkpoints/model.pt')\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxCJoixb50Ji"
      },
      "source": [
        "def to_tensor(data):\n",
        "  if np.iscomplexobj(data):\n",
        "      data = np.stack((data.real, data.imag), axis=-1)\n",
        "  return torch.from_numpy(data)\n",
        "\n",
        "\n",
        "def apply_mask(data, mask_func, seed=None):\n",
        "  shape = np.array(data.shape)\n",
        "  shape[:-3] = 1\n",
        "  mask = mask_func(shape, seed)\n",
        "  return torch.where(mask == 0, torch.Tensor([0]), data), mask\n",
        "\n",
        "\n",
        "def fft2(data):\n",
        "  assert data.size(-1) == 2\n",
        "  data = ifftshift(data, dim=(-3, -2))\n",
        "  data = torch.fft.fft(data, dim=2, norm='backward')\n",
        "  data = fftshift(data, dim=(-3, -2))\n",
        "  return data\n",
        "\n",
        "\n",
        "def ifft2(data):\n",
        "  assert data.size(-1) == 2\n",
        "  data = ifftshift(data, dim=(-3, -2))\n",
        "  data = torch.fft.ifft(data, dim=2, norm='backward')\n",
        "  data = fftshift(data, dim=(-3, -2))\n",
        "  return data\n",
        "\n",
        "\n",
        "def complex_abs(data):\n",
        "  assert data.size(-1) == 2\n",
        "  return (data ** 2).sum(dim=-1).sqrt()\n",
        "\n",
        "\n",
        "def root_sum_of_squares(data, dim=0):\n",
        "  return torch.sqrt((data ** 2).sum(dim))\n",
        "\n",
        "\n",
        "def center_crop(data, shape):\n",
        "  assert 0 < shape[0] <= data.shape[-2]\n",
        "  assert 0 < shape[1] <= data.shape[-1]\n",
        "  w_from = (data.shape[-2] - shape[0]) // 2\n",
        "  h_from = (data.shape[-1] - shape[1]) // 2\n",
        "  w_to = w_from + shape[0]\n",
        "  h_to = h_from + shape[1]\n",
        "  return data[..., w_from:w_to, h_from:h_to]\n",
        "\n",
        "\n",
        "def complex_center_crop(data, shape):\n",
        "  assert 0 < shape[0] <= data.shape[-3]\n",
        "  assert 0 < shape[1] <= data.shape[-2]\n",
        "  w_from = (data.shape[-3] - shape[0]) // 2\n",
        "  h_from = (data.shape[-2] - shape[1]) // 2\n",
        "  w_to = w_from + shape[0]\n",
        "  h_to = h_from + shape[1]\n",
        "  return data[..., w_from:w_to, h_from:h_to, :]\n",
        "\n",
        "\n",
        "def normalize(data, mean, stddev, eps=0.):\n",
        "  return (data - mean) / (stddev + eps)\n",
        "\n",
        "\n",
        "def normalize_instance(data, eps=0.):\n",
        "  mean = data.mean()\n",
        "  std = data.std()\n",
        "  return normalize(data, mean, std, eps), mean, std\n",
        "\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "def roll(x, shift, dim):\n",
        "  if isinstance(shift, (tuple, list)):\n",
        "      assert len(shift) == len(dim)\n",
        "      for s, d in zip(shift, dim):\n",
        "          x = roll(x, s, d)\n",
        "      return x\n",
        "  shift = shift % x.size(dim)\n",
        "  if shift == 0:\n",
        "      return x\n",
        "  left = x.narrow(dim, 0, x.size(dim) - shift)\n",
        "  right = x.narrow(dim, x.size(dim) - shift, shift)\n",
        "  return torch.cat((right, left), dim=dim)\n",
        "\n",
        "\n",
        "def fftshift(x, dim=None):\n",
        "  if dim is None:\n",
        "      dim = tuple(range(x.dim()))\n",
        "      shift = [dim // 2 for dim in x.shape]\n",
        "  elif isinstance(dim, int):\n",
        "      shift = x.shape[dim] // 2\n",
        "  else:\n",
        "      shift = [x.shape[i] // 2 for i in dim]\n",
        "  return roll(x, shift, dim)\n",
        "\n",
        "\n",
        "def ifftshift(x, dim=None):\n",
        "  if dim is None:\n",
        "      dim = tuple(range(x.dim()))\n",
        "      shift = [(dim + 1) // 2 for dim in x.shape]\n",
        "  elif isinstance(dim, int):\n",
        "      shift = (x.shape[dim] + 1) // 2\n",
        "  else:\n",
        "      shift = [(x.shape[i] + 1) // 2 for i in dim]\n",
        "  return roll(x, shift, dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPF3JR2M6X1a"
      },
      "source": [
        "class SliceData(Dataset):\n",
        "\n",
        "  def __init__(self, root, transform, challenge, sample_rate=1):\n",
        "    if challenge not in ('singlecoil', 'multicoil'):\n",
        "        raise ValueError('challenge should be either \"singlecoil\" or \"multicoil\"')\n",
        "\n",
        "    self.transform = transform\n",
        "    self.recons_key = 'reconstruction_esc' if challenge == 'singlecoil' \\\n",
        "        else 'reconstruction_rss'\n",
        "\n",
        "    self.examples = []\n",
        "    files = list(pathlib.Path(root).iterdir())\n",
        "    if sample_rate < 1:\n",
        "        random.shuffle(files)\n",
        "        num_files = round(len(files) * sample_rate)\n",
        "        files = files[:num_files]\n",
        "    for fname in sorted(files):\n",
        "      try:\n",
        "        kspace = h5py.File(fname, 'r')['kspace']\n",
        "        num_slices = kspace.shape[0]\n",
        "        self.examples += [(fname, slice) for slice in range(num_slices)]\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    fname, slice = self.examples[i]\n",
        "    with h5py.File(fname, 'r') as data:\n",
        "        kspace = data['kspace'][slice]\n",
        "        target = data[self.recons_key][slice] if self.recons_key in data else None\n",
        "        return self.transform(kspace, target, data.attrs, fname.name, slice)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kip-N2Mc624Y"
      },
      "source": [
        "class MaskFunc:\n",
        "\n",
        "  def __init__(self, center_fractions, accelerations):\n",
        "    if len(center_fractions) != len(accelerations):\n",
        "        raise ValueError('Number of center fractions should match number of accelerations')\n",
        "\n",
        "    self.center_fractions = center_fractions\n",
        "    self.accelerations = accelerations\n",
        "    self.rng = np.random.RandomState()\n",
        "\n",
        "  def __call__(self, shape, seed=None):\n",
        "    if len(shape) < 3:\n",
        "        raise ValueError('Shape should have 3 or more dimensions')\n",
        "\n",
        "    self.rng.seed(seed)\n",
        "    num_cols = shape[-2]\n",
        "\n",
        "    choice = self.rng.randint(0, len(self.accelerations))\n",
        "    center_fraction = self.center_fractions[choice]\n",
        "    acceleration = self.accelerations[choice]\n",
        "\n",
        "    # Create the mask\n",
        "    num_low_freqs = int(round(num_cols * center_fraction))\n",
        "    prob = (num_cols / acceleration - num_low_freqs) / (num_cols - num_low_freqs)\n",
        "    mask = self.rng.uniform(size=num_cols) < prob\n",
        "    pad = (num_cols - num_low_freqs + 1) // 2\n",
        "    mask[pad:pad + num_low_freqs] = True\n",
        "\n",
        "    # Reshape the mask\n",
        "    mask_shape = [1 for _ in shape]\n",
        "    mask_shape[-2] = num_cols\n",
        "    mask = torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n",
        "\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im61771U7BRx"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, in_chans, out_chans, drop_prob):\n",
        "      super().__init__()\n",
        "\n",
        "      self.in_chans = in_chans\n",
        "      self.out_chans = out_chans\n",
        "      self.drop_prob = drop_prob\n",
        "\n",
        "      self.layers = nn.Sequential(\n",
        "          nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1),\n",
        "          nn.InstanceNorm2d(out_chans),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout2d(drop_prob),\n",
        "          nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1),\n",
        "          nn.InstanceNorm2d(out_chans),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout2d(drop_prob)\n",
        "      )\n",
        "\n",
        "  def forward(self, input):\n",
        "      return self.layers(input)\n",
        "\n",
        "  def __repr__(self):\n",
        "      return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, ' \\\n",
        "          f'drop_prob={self.drop_prob})'\n",
        "\n",
        "\n",
        "class UnetModel(nn.Module):\n",
        "\n",
        "  def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob):\n",
        "      super().__init__()\n",
        "\n",
        "      self.in_chans = in_chans\n",
        "      self.out_chans = out_chans\n",
        "      self.chans = chans\n",
        "      self.num_pool_layers = num_pool_layers\n",
        "      self.drop_prob = drop_prob\n",
        "\n",
        "      self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
        "      ch = chans\n",
        "      for i in range(num_pool_layers - 1):\n",
        "          self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob)]\n",
        "          ch *= 2\n",
        "      self.conv = ConvBlock(ch, ch, drop_prob)\n",
        "\n",
        "      self.up_sample_layers = nn.ModuleList()\n",
        "      for i in range(num_pool_layers - 1):\n",
        "          self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob)]\n",
        "          ch //= 2\n",
        "      self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob)]\n",
        "      self.conv2 = nn.Sequential(\n",
        "          nn.Conv2d(ch, ch // 2, kernel_size=1),\n",
        "          nn.Conv2d(ch // 2, out_chans, kernel_size=1),\n",
        "          nn.Conv2d(out_chans, out_chans, kernel_size=1),\n",
        "      )\n",
        "\n",
        "  def forward(self, input):\n",
        "      stack = []\n",
        "      output = input\n",
        "      # Apply down-sampling layers\n",
        "      for layer in self.down_sample_layers:\n",
        "          output = layer(output)\n",
        "          stack.append(output)\n",
        "          output = F.max_pool2d(output, kernel_size=2)\n",
        "\n",
        "      output = self.conv(output)\n",
        "\n",
        "      # Apply up-sampling layers\n",
        "      for layer in self.up_sample_layers:\n",
        "          output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "          output = torch.cat([output, stack.pop()], dim=1)\n",
        "          output = layer(output)\n",
        "      return self.conv2(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ6-x09W7mNK"
      },
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLybyOJw8G3f"
      },
      "source": [
        "class DataTransform:\n",
        "\n",
        "  def __init__(self, mask_func, resolution, which_challenge, use_seed=True):\n",
        "    if which_challenge not in ('singlecoil', 'multicoil'):\n",
        "        raise ValueError(f'Challenge should either be \"singlecoil\" or \"multicoil\"')\n",
        "    self.mask_func = mask_func\n",
        "    self.resolution = resolution\n",
        "    self.which_challenge = which_challenge\n",
        "    self.use_seed = use_seed\n",
        "\n",
        "  def __call__(self, kspace, target, attrs, fname, slice):\n",
        "    kspace = to_tensor(kspace)\n",
        "    # Apply mask\n",
        "    seed = None if not self.use_seed else tuple(map(ord, fname))\n",
        "    masked_kspace, mask = apply_mask(kspace, self.mask_func, seed)\n",
        "    # Inverse Fourier Transform to get zero filled solution\n",
        "    image = ifft2(masked_kspace)\n",
        "    # Crop input image\n",
        "    image = complex_center_crop(image, (self.resolution, self.resolution))\n",
        "    # Absolute value\n",
        "    # image = complex_abs(image)\n",
        "    image = image.abs()\n",
        "    # Apply Root-Sum-of-Squares if multicoil data\n",
        "    if self.which_challenge == 'multicoil':\n",
        "        image = root_sum_of_squares(image)\n",
        "    # Normalize input\n",
        "    image, mean, std = normalize_instance(image, eps=1e-11)\n",
        "    image = torch.clamp(image, min=-6, max=6)\n",
        "\n",
        "    target = to_tensor(target)\n",
        "    # Normalize target\n",
        "    target = normalize(target, mean, std, eps=1e-11)\n",
        "    target = target.clamp(-6, 6)\n",
        "    return image, target, mean, std, attrs['norm'].astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcEIBqwm8ZiX"
      },
      "source": [
        "def create_datasets(args):\n",
        "  train_mask = MaskFunc(args['center_fractions'], args['accelerations'])\n",
        "  dev_mask = MaskFunc(args['center_fractions'], args['accelerations'])\n",
        "\n",
        "  train_data = SliceData(\n",
        "      root=args['data_path'],\n",
        "      transform=DataTransform(train_mask, args['resolution'], args['challenge']),\n",
        "      sample_rate=args['sample_rate'],\n",
        "      challenge=args['challenge']\n",
        "  )\n",
        "  dev_data = SliceData(\n",
        "      root=args['data_path'],\n",
        "      transform=DataTransform(dev_mask, args['resolution'], args['challenge'], use_seed=True),\n",
        "      sample_rate=args['sample_rate'],\n",
        "      challenge=args['challenge'],\n",
        "  )\n",
        "  return dev_data, train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PPTUV9y9jxQ"
      },
      "source": [
        "def create_data_loaders(args):\n",
        "  dev_data, train_data = create_datasets(args)\n",
        "  display_data = [dev_data[i] for i in range(0, len(dev_data), len(dev_data) // 16)]\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      dataset=train_data,\n",
        "      batch_size=args['batch_size'],\n",
        "      shuffle=True,\n",
        "      num_workers=8,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  dev_loader = DataLoader(\n",
        "      dataset=dev_data,\n",
        "      batch_size=args['batch_size'],\n",
        "      num_workers=8,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  display_loader = DataLoader(\n",
        "      dataset=display_data,\n",
        "      batch_size=16,\n",
        "      num_workers=8,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  return train_loader, dev_loader, display_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTEn4HxU-LGY"
      },
      "source": [
        "def train_epoch(args, epoch, model, data_loader, optimizer, writer):\n",
        "  model.train()\n",
        "  avg_loss = 0.\n",
        "  start_epoch = start_iter = time.perf_counter()\n",
        "  global_step = epoch * len(data_loader)\n",
        "  for iter, data in enumerate(data_loader):\n",
        "      input, target, mean, std, norm = data\n",
        "      input = input.unsqueeze(1).to(args['device'])\n",
        "      target = target.to(args['device'])\n",
        "      input_new = (input[:, :, :, :, 0] + input[:, :, :, :, 1]) / 2\n",
        "      output = model(input_new).squeeze(1)\n",
        "      loss = F.l1_loss(output, target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
        "      writer.add_scalar('TrainLoss', loss.item(), global_step + iter)\n",
        "\n",
        "      num_epochs = args['num_epochs']\n",
        "      if iter % args['report_interval'] == 0:\n",
        "          logging.info(\n",
        "              f'Epoch = [{epoch:3d}/{num_epochs:3d}] '\n",
        "              f'Iter = [{iter:4d}/{len(data_loader):4d}] '\n",
        "              f'Loss = {loss.item():.4g} Avg Loss = {avg_loss:.4g} '\n",
        "              f'Time = {time.perf_counter() - start_iter:.4f}s',\n",
        "          )\n",
        "      start_iter = time.perf_counter()\n",
        "  return avg_loss, time.perf_counter() - start_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgb2faIS_RYk"
      },
      "source": [
        "def evaluate(args, epoch, model, data_loader, writer):\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  start = time.perf_counter()\n",
        "  with torch.no_grad():\n",
        "      for iter, data in enumerate(data_loader):\n",
        "          input, target, mean, std, norm = data\n",
        "          input = input.unsqueeze(1).to(args['device'])\n",
        "          target = target.to(args['device'])\n",
        "          input_new = (input[:, :, :, :, 0] + input[:, :, :, :, 1]) / 2\n",
        "          output = model(input_new).squeeze(1)\n",
        "\n",
        "          mean = mean.unsqueeze(1).unsqueeze(2).to(args['device'])\n",
        "          std = std.unsqueeze(1).unsqueeze(2).to(args['device'])\n",
        "          target = target * std + mean\n",
        "          output = output * std + mean\n",
        "\n",
        "          norm = norm.unsqueeze(1).unsqueeze(2).to(args['device'])\n",
        "          loss = F.mse_loss(output / norm, target / norm, size_average=False)\n",
        "          losses.append(loss.item())\n",
        "      writer.add_scalar('Dev_Loss', np.mean(losses), epoch)\n",
        "  return np.mean(losses), time.perf_counter() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc8PnP5L_wh-"
      },
      "source": [
        "def visualize(args, epoch, model, data_loader, writer):\n",
        "  def save_image(image, tag):\n",
        "    image -= image.min()\n",
        "    image /= image.max()\n",
        "    grid = torchvision.utils.make_grid(image, nrow=4, pad_value=1)\n",
        "    writer.add_image(tag, grid, epoch)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for iter, data in enumerate(data_loader):\n",
        "      input, target, mean, std, norm = data\n",
        "      input = input.unsqueeze(1).to(args['device'])\n",
        "      target = target.unsqueeze(1).to(args['device'])\n",
        "      input_new = (input[:, :, :, :, 0] + input[:, :, :, :, 1]) / 2\n",
        "      output = model(input_new)\n",
        "      save_image(target, 'Target')\n",
        "      save_image(output, 'Reconstruction')\n",
        "      save_image(torch.abs(target - output), 'Error')\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrxVO5SxAKdG"
      },
      "source": [
        "def save_model(args, exp_dir, epoch, model, optimizer, best_dev_loss, is_new_best):\n",
        "  torch.save(\n",
        "      {\n",
        "          'epoch': epoch,\n",
        "          'args': args,\n",
        "          'model': model.state_dict(),\n",
        "          'optimizer': optimizer.state_dict(),\n",
        "          'best_dev_loss': best_dev_loss,\n",
        "          'exp_dir': exp_dir\n",
        "      },\n",
        "      f=exp_dir / 'model.pt'\n",
        "  )\n",
        "  if is_new_best:\n",
        "      shutil.copyfile(exp_dir / 'model.pt', exp_dir / 'best_model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjg115MbAULE"
      },
      "source": [
        "def build_model(args):\n",
        "  model = UnetModel(\n",
        "      in_chans=1,\n",
        "      out_chans=1,\n",
        "      chans=args['num_chans'],\n",
        "      num_pool_layers=args['num_pools'],\n",
        "      drop_prob=args['drop_prob']\n",
        "  ).to(args['device'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NyKNlbzAeDc"
      },
      "source": [
        "def load_model(checkpoint_file):\n",
        "  checkpoint = torch.load(checkpoint_file)\n",
        "  args = checkpoint['args']\n",
        "  model = build_model(args)\n",
        "  if args['data_parallel']:\n",
        "      model = torch.nn.DataParallel(model)\n",
        "  model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "  optimizer = build_optim(args, model.parameters())\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  return checkpoint, model, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL9VbIAJAlvN"
      },
      "source": [
        "def build_optim(args, params):\n",
        "  optimizer = torch.optim.RMSprop(params, args['lr'], weight_decay=args['weight_decay'])\n",
        "  return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKecqhtLAsIy"
      },
      "source": [
        "def main(args):\n",
        "    args['exp_dir'].mkdir(parents=True, exist_ok=True)\n",
        "    writer = SummaryWriter(log_dir=args['exp_dir'] / 'summary')\n",
        "\n",
        "    if args['resume']:\n",
        "        checkpoint, model, optimizer = load_model(args['checkpoint'])\n",
        "        args = checkpoint['args']\n",
        "        best_dev_loss = checkpoint['best_dev_loss']\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        del checkpoint\n",
        "    else:\n",
        "        model = build_model(args)\n",
        "        if args['data_parallel']:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "        optimizer = build_optim(args, model.parameters())\n",
        "        best_dev_loss = 1e9\n",
        "        start_epoch = 0\n",
        "    logging.info(args)\n",
        "    logging.info(model)\n",
        "\n",
        "    train_loader, dev_loader, display_loader = create_data_loaders(args)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args['lr_step_size'], args['lr_gamma'])\n",
        "\n",
        "    for epoch in range(start_epoch, args['num_epochs']):\n",
        "        scheduler.step(epoch)\n",
        "        train_loss, train_time = train_epoch(args, epoch, model, train_loader, optimizer, writer)\n",
        "        dev_loss, dev_time = evaluate(args, epoch, model, dev_loader, writer)\n",
        "        visualize(args, epoch, model, display_loader, writer)\n",
        "\n",
        "        is_new_best = dev_loss < best_dev_loss\n",
        "        best_dev_loss = min(best_dev_loss, dev_loss)\n",
        "        save_model(args, args['exp_dir'], epoch, model, optimizer, best_dev_loss, is_new_best)\n",
        "        num_epochs = args['num_epochs']\n",
        "        logging.info(\n",
        "            f'Epoch = [{epoch:4d}/{num_epochs:4d}] TrainLoss = {train_loss:.4g} '\n",
        "            f'DevLoss = {dev_loss:.4g} TrainTime = {train_time:.4f}s DevTime = {dev_time:.4f}s',\n",
        "        )\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LILYyNtuzFL3",
        "outputId": "1bc2ab3f-94cf-4e90-e279-d73c61a89b4d"
      },
      "source": [
        "random.seed(args['seed'])\n",
        "np.random.seed(args['seed'])\n",
        "torch.manual_seed(args['seed'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f63a93c45f0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3By_4XTAqyWH",
        "outputId": "445973ef-437c-4d08-8f1f-9440c23054b9"
      },
      "source": [
        "args['exp_dir'].mkdir(parents=True, exist_ok=True)\n",
        "writer = SummaryWriter(log_dir=args['exp_dir'] / 'summary')\n",
        "\n",
        "if args['resume']:\n",
        "    checkpoint, model, optimizer = load_model(args['checkpoint'])\n",
        "    args = checkpoint['args']\n",
        "    best_dev_loss = checkpoint['best_dev_loss']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    del checkpoint\n",
        "else:\n",
        "    model = build_model(args)\n",
        "    if args['data_parallel']:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "    optimizer = build_optim(args, model.parameters())\n",
        "    best_dev_loss = 1e9\n",
        "    start_epoch = 0\n",
        "logging.info(args)\n",
        "logging.info(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:{'seed': 42, 'resolution': 320, 'challenge': 'singlecoil', 'data_path': PosixPath('/content/drive/MyDrive/Dataset'), 'sample_rate': 1.0, 'accelerations': [4, 8], 'center_fractions': [0.08, 0.04], 'num_pools': 4, 'drop_prob': 0.0, 'num_chans': 32, 'batch_size': 16, 'num_epochs': 2, 'lr': 0.001, 'lr_step_size': 40, 'lr_gamma': 0.1, 'weight_decay': 0.0, 'report_interval': 100, 'data_parallel': False, 'device': 'cuda', 'exp_dir': PosixPath('/content/drive/MyDrive/checkpoints'), 'resume': False, 'checkpoint': ''}\n",
            "INFO:root:UnetModel(\n",
            "  (down_sample_layers): ModuleList(\n",
            "    (0): ConvBlock(in_chans=1, out_chans=32, drop_prob=0.0)\n",
            "    (1): ConvBlock(in_chans=32, out_chans=64, drop_prob=0.0)\n",
            "    (2): ConvBlock(in_chans=64, out_chans=128, drop_prob=0.0)\n",
            "    (3): ConvBlock(in_chans=128, out_chans=256, drop_prob=0.0)\n",
            "  )\n",
            "  (conv): ConvBlock(in_chans=256, out_chans=256, drop_prob=0.0)\n",
            "  (up_sample_layers): ModuleList(\n",
            "    (0): ConvBlock(in_chans=512, out_chans=128, drop_prob=0.0)\n",
            "    (1): ConvBlock(in_chans=256, out_chans=64, drop_prob=0.0)\n",
            "    (2): ConvBlock(in_chans=128, out_chans=32, drop_prob=0.0)\n",
            "    (3): ConvBlock(in_chans=64, out_chans=32, drop_prob=0.0)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyZdl7rmq725",
        "outputId": "682b041b-cb13-4ff6-9da6-e660f68f95d0"
      },
      "source": [
        "train_loader, dev_loader, display_loader = create_data_loaders(args)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args['lr_step_size'], args['lr_gamma'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cdg79rMvL6g"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbFKsPKcq_5Q",
        "outputId": "22d3313a-4f10-49db-9974-c573a5b38b42"
      },
      "source": [
        "for epoch in range(start_epoch, args['num_epochs']):\n",
        "    scheduler.step(epoch)\n",
        "    train_loss, train_time = train_epoch(args, epoch, model, train_loader, optimizer, writer)\n",
        "    dev_loss, dev_time = evaluate(args, epoch, model, dev_loader, writer)\n",
        "    visualize(args, epoch, model, display_loader, writer)\n",
        "\n",
        "    is_new_best = dev_loss < best_dev_loss\n",
        "    best_dev_loss = min(best_dev_loss, dev_loss)\n",
        "    save_model(args, args['exp_dir'], epoch, model, optimizer, best_dev_loss, is_new_best)\n",
        "    num_epochs = args['num_epochs']\n",
        "    logging.info(\n",
        "        f'Epoch = [{epoch:4d}/{num_epochs:4d}] TrainLoss = {train_loss:.4g} '\n",
        "        f'DevLoss = {dev_loss:.4g} TrainTime = {train_time:.4f}s DevTime = {dev_time:.4f}s',\n",
        "    )\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "INFO:root:Epoch = [  0/  2] Iter = [   0/ 326] Loss = 0.8354 Avg Loss = 0.8354 Time = 132.0261s\n",
            "INFO:root:Epoch = [  0/  2] Iter = [ 100/ 326] Loss = 0.9327 Avg Loss = 0.8192 Time = 0.9933s\n",
            "INFO:root:Epoch = [  0/  2] Iter = [ 200/ 326] Loss = 0.7538 Avg Loss = 0.8057 Time = 0.9958s\n",
            "INFO:root:Epoch = [  0/  2] Iter = [ 300/ 326] Loss = 0.9085 Avg Loss = 0.79 Time = 0.9975s\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "INFO:root:Epoch = [   0/   2] TrainLoss = 0.7879 DevLoss = 0.1438 TrainTime = 4803.1036s DevTime = 292.0575s\n",
            "INFO:root:Epoch = [  1/  2] Iter = [   0/ 326] Loss = 0.7447 Avg Loss = 0.7447 Time = 131.8316s\n",
            "INFO:root:Epoch = [  1/  2] Iter = [ 100/ 326] Loss = 0.7409 Avg Loss = 0.7558 Time = 0.9945s\n",
            "INFO:root:Epoch = [  1/  2] Iter = [ 200/ 326] Loss = 0.7183 Avg Loss = 0.756 Time = 0.9928s\n",
            "INFO:root:Epoch = [  1/  2] Iter = [ 300/ 326] Loss = 0.7915 Avg Loss = 0.7513 Time = 0.9907s\n",
            "INFO:root:Epoch = [   1/   2] TrainLoss = 0.7561 DevLoss = 0.1201 TrainTime = 4840.9482s DevTime = 291.9463s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkaIsbtVBTS9"
      },
      "source": [
        "# main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV8sta8mDXKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a2a727-2ef6-45c9-fa24-b3456e8f2f7c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov 30 06:27:25 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    85W / 149W |   6430MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}